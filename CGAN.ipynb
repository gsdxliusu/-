{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1导入库函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.0.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TF version:\",tf.__version__)\n",
    "#检测TensorFlow是否支持GPU\n",
    "print(\"GPU is\",\"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import glob\n",
    " \n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入准备好的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 使用Fashion MNIST进行GAN训练，生成器将生成类似于FashionMNIST的数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#导入fashion-mnist数据集\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images,train_labels),(test_images,test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0],28,28,1).astype('float32')\n",
    "train_images = (train_images - 127.5)/127.5  #将图片标准化到[-1,1]区间内\n",
    "\n",
    "print(train_images.shape)\n",
    "#将标签进行独热编码\n",
    "train_labels = tf.one_hot(train_labels,depth = 10)\n",
    "train_labels = tf.cast(train_labels,tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer_size = 60000\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#批量化和打乱数据\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images,train_labels)).shuffle(buffer_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 定义生成器 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256,use_bias = False,input_shape = (110,))) #因为加入了标签信息，input_shape发送改变\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    \n",
    "    model.add(layers.Reshape((7,7,256)))\n",
    "    assert model.output_shape == (None,7,7,256)  #注意： batch size未限制\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(128,(5,5),strides = (1,1),padding = 'same',use_bias = False))\n",
    "    assert model.output_shape == (None,7,7,128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64,(5,5),strides = (2,2),padding = 'same',use_bias = False))\n",
    "    assert model.output_shape == (None,14,14,64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(1,(5,5),strides = (2,2),padding = 'same',use_bias = False,activation = 'tanh'))\n",
    "    assert model.output_shape == (None,28,28,1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 查看生成器网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12544)             1379840   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 7, 7, 128)         819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         1600      \n",
      "=================================================================\n",
      "Total params: 2,456,384\n",
      "Trainable params: 2,430,912\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2定义判别器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 判别器可以视作一个CNN分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64,(5,5),strides = (2,2),padding = 'same',input_shape = [28,28,11])) # input_shape加入了标签信息   \n",
    "    \n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128,(5,5),strides = (2,2),padding = 'same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 查看判别器网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        17664     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6273      \n",
      "=================================================================\n",
      "Total params: 228,865\n",
      "Trainable params: 228,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## 2.3 定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 判别器损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output,fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output),real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output),fake_output)\n",
    "    total_loss = real_loss+fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 生成器损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output),fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 两者的优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 保存检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer,discriminator_optimizer = discriminator_optimizer,generator = generator,discriminator = discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 定义超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 定义超参数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ####产生随机种子作为输入 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100, 10)\n",
      "(100, 10)\n",
      "(100, 110)\n"
     ]
    }
   ],
   "source": [
    "#后面将重复使用该种子 （因此在动画GIF中更容易可视化进度\n",
    "seed = tf.random.normal([num_examples_to_generate,noise_dim])\n",
    "print(seed.shape)\n",
    "\n",
    "#有规律设置标签\n",
    "labels = [i%10 for i in range(num_examples_to_generate)] #舒适化标签向量\n",
    "\n",
    "labels = tf.one_hot(labels,depth = 10) #将标签独热编码\n",
    "print(labels.shape)\n",
    "labels = tf.cast(labels,tf.float32) #转换为tf.float32类型\n",
    "print(labels.shape)\n",
    "seed = tf.concat([seed,labels],1) #和图像数据连接起来作为后面的输入数据\n",
    "print(seed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 定义训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 训练在生成器接收到一个种子作为输入时开始，用于生产一张图片，判别器随后被用于区分真实图片（自选训练集）和伪造图片（由生成器生成）。 针对这里的每一个模型都计算损失函数，并且计算梯度用于更新生成器与判别器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#  注意  'tf.function'的使用，该注解使函数被“编译”\n",
    "@tf.function\n",
    "def train_step(data_batch):\n",
    "    \n",
    "    \n",
    "    images = data_batch[0] # 图像数据\n",
    "    labels = data_batch[1] # 标签数据\n",
    "    print(labels.shape)\n",
    "    print(images.shape)\n",
    "    with tf.GradientTape() as gen_tape,tf.GradientTape() as disc_tape:\n",
    "        noise = tf.random.normal([images.get_shape()[0], noise_dim]) # 产生噪音图像\n",
    "        print(images.get_shape()[0])\n",
    "\n",
    "        print(noise.shape)\n",
    "        noise_input = tf.concat([noise,labels],1)        # 噪音图像数据连接标签数据\n",
    "        generated_images = generator(noise_input,training = True) # 生成图像\n",
    "        \n",
    "        labels_input = tf.reshape(labels,[images.get_shape()[0],1,1,10])\n",
    "        \n",
    "        images_input = tf.concat([images,labels_input*tf.ones([images.get_shape()[0],28,28,10])],3)\n",
    "        # 生成图像数据连接上标签数据\n",
    "        generated_input = tf.concat([generated_images,labels_input*tf.ones([images.get_shape()[0],28,28,10])],3)\n",
    "        \n",
    "        real_output = discriminator(images_input,training = True) # 真实图像的判决结果\n",
    "        fake_output = discriminator(generated_input,training = True) # 生成图像的判决结果\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)  # 计算生成器损失\n",
    "        disc_loss = discriminator_loss(real_output,fake_output) # 计算判别器损失\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss,generator.trainable_variables) # 求梯度\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
    "    \n",
    "      # 优化变量\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator,generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 定义训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dataset,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for data_batch in dataset:\n",
    "            train_step(data_batch)\n",
    "        \n",
    "        #生成图片\n",
    "        display.clear_output(wait = True)\n",
    "        generate_and_save_images(generator,epoch+1,seed)\n",
    "        \n",
    "        #每5 epochs进行一次存储\n",
    "        if(epoch+1) % 5 ==  0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "        print('Time for epoch {} is {} sec'.format(epoch+1,time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 生成和保存图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model,epoch,test_input):\n",
    "    # 注意 'training' 设定值为False \n",
    "    # 因此，所有层都在推理模式下运行（batchnorm）。\n",
    "    \n",
    "    predictions = model(test_input,training = False)\n",
    "    \n",
    "    fig = plt.figure(figsize = (10,10))\n",
    "    \n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(10,10,i+1)\n",
    "        plt.imshow(predictions[i,:,:,0]*127.5+127.5,cmap = 'gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.savefig('Cimage_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 调用上面定义的train（）方法来同时训练生成器和判别器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 在训练之初，生成的图片看起来像是随机噪声。\n",
    "    #### 随着训练进行，生成的图片将越来越真实。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 10)\n",
      "(256, 28, 28, 1)\n",
      "256\n",
      "(256, 100)\n",
      "(256, 10)\n",
      "(256, 28, 28, 1)\n",
      "256\n",
      "(256, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "train(train_dataset,epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
